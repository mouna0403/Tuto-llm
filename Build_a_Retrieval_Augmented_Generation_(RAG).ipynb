{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGSK20Mn2Vq5nHJOT7qhr4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mouna0403/Tuto-llm/blob/main/Build_a_Retrieval_Augmented_Generation_(RAG).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YCGRavJ3cZ6c",
        "outputId": "ee52fcc9-2c9e-40e3-e9e2-91c353198e07"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.3.8-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (0.3.76)\n",
            "Collecting groq<1,>=0.30.0 (from langchain-groq)\n",
            "  Downloading groq-0.31.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (2.11.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (4.15.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (0.4.28)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain-groq) (25.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.30.0->langchain-groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.75->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain-groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain-groq) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain-groq) (2.5.0)\n",
            "Downloading langchain_groq-0.3.8-py3-none-any.whl (16 kB)\n",
            "Downloading groq-0.31.1-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain-groq\n",
            "Successfully installed groq-0.31.1 langchain-groq-0.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üöÄ Installation des d√©pendances\n",
        "!pip install langchain-ollama\n",
        "!pip install ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# D√©marrer le service Ollama en arri√®re-plan\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "# D√©marrer Ollama en arri√®re-plan\n",
        "ollama_process = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "time.sleep(5)  # Attendre que le service d√©marre\n",
        "\n",
        "# T√©l√©charger un mod√®le l√©ger (Llama 3.2 3B ou Mistral 7B)\n",
        "!ollama pull llama3.2:3b  # Mod√®le l√©ger et rapide\n",
        "# Alternatives :\n",
        "# !ollama pull mistral:7b\n",
        "# !ollama pull codellama:7b\n",
        "# !ollama pull phi3:mini\n",
        "\n",
        "print(\"‚úÖ Installation termin√©e!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ca3-OwCohLK_",
        "outputId": "b86e020f-d69e-42b7-a7c2-3235b86b658f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-ollama in /usr/local/lib/python3.12/dist-packages (0.3.8)\n",
            "Requirement already satisfied: ollama<1.0.0,>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-ollama) (0.6.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.76 in /usr/local/lib/python3.12/dist-packages (from langchain-ollama) (0.3.76)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (0.4.28)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (2.11.9)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama<1.0.0,>=0.5.3->langchain-ollama) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (1.3.1)\n",
            "Requirement already satisfied: ollama in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from ollama) (2.11.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "‚úÖ Installation termin√©e!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "y1BI__hHMzEh",
        "outputId": "be56c7ce-bc33-44b2-d120-6cfb46fd2c05"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.76)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.28)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tK78ucrb65A",
        "outputId": "af769b2d-90dc-4f8c-dad8-31ad960f22b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "\n",
        "token=getpass.getpass(\"Enter your Groq API token: \")\n",
        "os.environ[\"GROQ_API_KEY\"] = token\n",
        "\n",
        "model = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",  # ou autre mod√®le disponible\n",
        "    temperature=0.4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\"Hello guyz\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHnaxxaLcOeg",
        "outputId": "46772223-b844-494a-cc78-af59a7b71424"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 38, 'total_tokens': 56, 'completion_time': 0.021763744, 'prompt_time': 0.001900427, 'queue_time': 0.214352261, 'total_time': 0.023664171}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_3ddc9808b3', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--2c3606d8-ad42-4b3e-8487-7da94aca4e7f-0', usage_metadata={'input_tokens': 38, 'output_tokens': 18, 'total_tokens': 56})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U langchain-ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Lm_cHcfsfD7r",
        "outputId": "0ddbe2c1-e404-42d4-8710-6414c195ad25"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-ollama in /usr/local/lib/python3.12/dist-packages (0.3.8)\n",
            "Requirement already satisfied: ollama<1.0.0,>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-ollama) (0.6.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.76 in /usr/local/lib/python3.12/dist-packages (from langchain-ollama) (0.3.76)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (0.4.28)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (2.11.9)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama<1.0.0,>=0.5.3->langchain-ollama) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import OllamaEmbeddings\n",
        "\n",
        "embeddings = OllamaEmbeddings(model=\"llama3.2:3b\")\n"
      ],
      "metadata": {
        "id": "uWg-xBERc0Li"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU langchain-core"
      ],
      "metadata": {
        "id": "jMKtNxtrfCtG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ],
      "metadata": {
        "id": "JltX8egmfSOe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain_community\n",
        "!pip install -qU langchain-text-splitter\n",
        "!pip install -qU langchain-document-loaders\n",
        "!pip install -qU typing_extensions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "smEaFk5OfhBK",
        "outputId": "192b2141-b623-401d-aec3-3c56a9cdd59c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/2.5 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/64.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-text-splitter (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-text-splitter\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-document-loaders (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-document-loaders\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPuKcgNEgUKI",
        "outputId": "898e9c1f-9273-4557-f8ab-190a62f34bf2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import des biblioth√®ques n√©cessaires\n",
        "import bs4  # BeautifulSoup pour parser le HTML\n",
        "from langchain import hub  # Pour r√©cup√©rer des prompts depuis LangChain Hub\n",
        "from langchain_community.document_loaders import WebBaseLoader  # Loader pour documents web\n",
        "from langchain_core.documents import Document  # Classe Document de LangChain\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter  # Pour d√©couper le texte\n",
        "from langgraph.graph import START, StateGraph  # Pour cr√©er un graphe d'√©tats\n",
        "from typing_extensions import List, TypedDict  # Pour typer les structures de donn√©es\n",
        "\n",
        "# üîπ √âtape 1 : Initialisation du loader web\n",
        "print(\"üîπ √âtape 1 : Initialisation du loader web\")\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),  # URL du site √† scraper\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")  # On ne parse que ces classes CSS\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "# üîπ √âtape 2 : Chargement des documents depuis le web\n",
        "print(\"üîπ √âtape 2 : Chargement des documents depuis le web\")\n",
        "docs = loader.load()  # R√©cup√®re les contenus web comme objets Document\n",
        "print(f\"‚úÖ {len(docs)} document(s) charg√©(s)\")\n",
        "\n",
        "# üîπ √âtape 3 : D√©coupage des documents en chunks\n",
        "print(\"üîπ √âtape 3 : D√©coupage des documents en chunks\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "all_splits = text_splitter.split_documents(docs)  # D√©coupe chaque document en morceaux de 1000 caract√®res\n",
        "print(f\"‚úÖ {len(all_splits)} chunk(s) cr√©√©(s)\")\n",
        "\n",
        "# üîπ √âtape 4 : Indexation dans le vector store\n",
        "print(\"üîπ √âtape 4 : Indexation dans le vector store\")\n",
        "_ = vector_store.add_documents(documents=all_splits[:5])  # On indexe les 5 premiers chunks dans le vector store\n",
        "print(\"‚úÖ Indexation termin√©e\")\n",
        "\n",
        "# üîπ √âtape 5 : R√©cup√©ration du prompt depuis LangChain Hub\n",
        "print(\"üîπ √âtape 5 : R√©cup√©ration du prompt depuis LangChain Hub\")\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")  # R√©cup√®re un prompt pr√©d√©fini pour le RAG (Retrieval-Augmented Generation)\n",
        "print(\"‚úÖ Prompt r√©cup√©r√©\")\n",
        "\n",
        "# D√©finition de l'√©tat pour l'application\n",
        "class State(TypedDict):\n",
        "    question: str  # Question pos√©e par l'utilisateur\n",
        "    context: List[Document]  # Documents r√©cup√©r√©s pour r√©pondre\n",
        "    answer: str  # R√©ponse g√©n√©r√©e\n",
        "\n",
        "# üîπ √âtape 6 : D√©finition des √©tapes de l'application\n",
        "def retrieve(state: State):\n",
        "    \"\"\"R√©cup√®re les documents les plus similaires √† la question\"\"\"\n",
        "    print(\"üîπ √âtape 6 : Recherche de similarit√© dans le vector store\")\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "    print(f\"‚úÖ {len(retrieved_docs)} document(s) r√©cup√©r√©(s)\")\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "def generate(state: State):\n",
        "    \"\"\"G√©n√®re la r√©ponse √† partir des documents r√©cup√©r√©s et du prompt\"\"\"\n",
        "    print(\"üîπ √âtape 7 : G√©n√©ration de la r√©ponse par le mod√®le\")\n",
        "    # On concat√®ne le contenu des documents\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    # On invoque le prompt avec la question et le contexte\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    # On g√©n√®re la r√©ponse via le mod√®le\n",
        "    response = model.invoke(messages)\n",
        "    print(\"‚úÖ R√©ponse g√©n√©r√©e\")\n",
        "    return {\"answer\": response.content}\n",
        "\n",
        "# üîπ √âtape 8 : Compilation du graphe de l'application\n",
        "print(\"üîπ √âtape 8 : Compilation du graphe de l'application\")\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])  # On ajoute les √©tapes dans l'ordre\n",
        "graph_builder.add_edge(START, \"retrieve\")  # On relie le point de d√©part √† la premi√®re √©tape\n",
        "graph = graph_builder.compile()  # Compilation finale du graphe\n",
        "print(\"‚úÖ Graphe compil√©\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezHCiIIDfUsK",
        "outputId": "8231c27f-8242-4824-88e2-52cb7991e8e1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ √âtape 1 : Initialisation du loader web\n",
            "üîπ √âtape 2 : Chargement des documents depuis le web\n",
            "‚úÖ 1 document(s) charg√©(s)\n",
            "üîπ √âtape 3 : D√©coupage des documents en chunks\n",
            "‚úÖ 63 chunk(s) cr√©√©(s)\n",
            "üîπ √âtape 4 : Indexation dans le vector store\n",
            "‚úÖ Indexation termin√©e\n",
            "üîπ √âtape 5 : R√©cup√©ration du prompt depuis LangChain Hub\n",
            "‚úÖ Prompt r√©cup√©r√©\n",
            "üîπ √âtape 8 : Compilation du graphe de l'application\n",
            "‚úÖ Graphe compil√©\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlwNR9jufepA",
        "outputId": "e062059c-d4e9-4739-fb01-2842638d5101"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ √âtape 6 : Recherche de similarit√© dans le vector store\n",
            "‚úÖ 4 document(s) r√©cup√©r√©(s)\n",
            "üîπ √âtape 7 : G√©n√©ration de la r√©ponse par le mod√®le\n",
            "‚úÖ R√©ponse g√©n√©r√©e\n",
            "Task Decomposition is the process of breaking down a large task into smaller, manageable subgoals. It is a key component in planning, allowing agents to efficiently handle complex tasks. Chain of Thought (CoT) and Tree of Thoughts (Yao et al. 2023) are techniques used for task decomposition.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading doc"
      ],
      "metadata": {
        "id": "kCKjjk4zSTrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Only keep post title, headers, and content from the full HTML.\n",
        "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "assert len(docs) == 1\n",
        "print(f\"Total characters: {len(docs[0].page_content)}\")"
      ],
      "metadata": {
        "id": "zGDRb8bijHG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2755bc4b-3d4c-4a52-b790-f80a0d97b84f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 43047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoos4sXxSEb9",
        "outputId": "2707b625-bba8-4f36-bd7f-9b987cb213c6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "      LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Splitting documents\n"
      ],
      "metadata": {
        "id": "tTk7nbveSd_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
        "\n",
        "To handle this we‚Äôll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time."
      ],
      "metadata": {
        "id": "2hS9xyuVSmPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in the semantic search tutorial, we use a **RecursiveCharacterTextSplitter**, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases."
      ],
      "metadata": {
        "id": "VGqDK_xbSvPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # chunk size (characters)\n",
        "    chunk_overlap=200,  # chunk overlap (characters)\n",
        "    add_start_index=True,  # track index in original document\n",
        ")\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuZLtgvYSHO8",
        "outputId": "f47442e7-e1cf-42da-8c36-04857f0edbef"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split blog post into 63 sub-documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Storing documents: Indexing"
      ],
      "metadata": {
        "id": "hsz3y0kLS94c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_ids = vector_store.add_documents(documents=all_splits[:5])\n",
        "\n",
        "print(document_ids[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0Hgpa0uSpKF",
        "outputId": "6a0f0f99-ec71-469e-a5e6-75b48303fbb8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['27f410e2-1d36-4039-b507-0473c6372064', '73d387c1-b5da-4bf8-abab-48e025cd9f9a', '3b104567-312a-4618-a0d7-8e837672d998']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval and Generation"
      ],
      "metadata": {
        "id": "NT2DSQ8wTht4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
        "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "example_messages = prompt.invoke(\n",
        "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
        ").to_messages()\n",
        "\n",
        "assert len(example_messages) == 1\n",
        "print(example_messages[0].content) #We‚Äôll use a prompt for RAG that is checked into the LangChain prompt hub (here)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtcsEH5ATBmQ",
        "outputId": "7ae994f0-4d71-4fac-bee0-bd77a1655821"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: (question goes here) \n",
            "Context: (context goes here) \n",
            "Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use LangGraph to tie together the retrieval and generation steps into a single application. This will bring a number of benefits:\n",
        "\n",
        "\n",
        "We can define our application logic once and automatically support\n",
        "multiple invocation modes, including streaming, async, and batched calls.\n",
        "We get streamlined deployments via LangGraph Platform.\n",
        "LangSmith will automatically trace the steps of our application together.\n",
        "We can easily add key features to our application, including persistence and human-in-the-loop approval, with minimal code changes."
      ],
      "metadata": {
        "id": "9Co6dKUGUmn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use LangGraph, we need to define three things:\n",
        "\n",
        "The state of our application;\n",
        "The nodes of our application (i.e., application steps);\n",
        "The \"control flow\" of our application (e.g., the ordering of the steps)."
      ],
      "metadata": {
        "id": "BvrA3fqPVQKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from typing_extensions import List, TypedDict\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str"
      ],
      "metadata": {
        "id": "auWuxJA-UdOH"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(state: State):\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = custom_rag_prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = model.invoke(messages)\n",
        "    return {\"answer\": response.content}"
      ],
      "metadata": {
        "id": "B4ZjxlKCVcaE"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vNropqShVgSj"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "JFwo15kfVc2e"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
        "\n",
        "print(f\"Context: {result['context']}\\n\\n\")\n",
        "print(f\"Answer: {result['answer']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLfAmsIhVfoc",
        "outputId": "ad6c7361-2ffd-4273-9491-5e3d67da0065"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: [Document(id='243746bd-6da2-4988-af7b-88ffb84523db', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.'), Document(id='73d387c1-b5da-4bf8-abab-48e025cd9f9a', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 971}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.'), Document(id='35bff5d2-2aac-4a48-bce8-d9aec067e45e', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'), Document(id='3b104567-312a-4618-a0d7-8e837672d998', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.')]\n",
            "\n",
            "\n",
            "Answer: Task Decomposition is a process used by the agent to break down complicated tasks into smaller, manageable steps. This is achieved through techniques like Chain of Thought (CoT) and Tree of Thoughts, which help the model plan ahead and think step by step.\n",
            "\n",
            "These techniques allow the model to decompose hard tasks into multiple manageable tasks, making it easier to solve complex problems.\n",
            "\n",
            "Thanks for asking!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for step in graph.stream(\n",
        "    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"\n",
        "):\n",
        "    print(f\"{step}\\n\\n----------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMpesZQwVknh",
        "outputId": "3e1f2a77-6302-47a9-f635-fb031fb0acde"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'retrieve': {'context': [Document(id='243746bd-6da2-4988-af7b-88ffb84523db', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.'), Document(id='73d387c1-b5da-4bf8-abab-48e025cd9f9a', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 971}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.'), Document(id='35bff5d2-2aac-4a48-bce8-d9aec067e45e', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'), Document(id='3b104567-312a-4618-a0d7-8e837672d998', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.')]}}\n",
            "\n",
            "----------------\n",
            "\n",
            "{'generate': {'answer': 'Task Decomposition is a process that breaks down complex tasks into smaller, manageable steps. It involves decomposing a problem into multiple thought steps and generating multiple thoughts per step, creating a tree structure to plan and organize the task.\\n\\nThanks for asking!'}}\n",
            "\n",
            "----------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "custom_rag_prompt = PromptTemplate.from_template(template) #Aulieu du prompt from HUB I CAN CUSTOMIZE MINE"
      ],
      "metadata": {
        "id": "mVemyu4pV4I0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query analysis"
      ],
      "metadata": {
        "id": "G6nnGk7IYf1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we are executing the retrieval using the raw input query. However, there are some advantages to allowing a model to generate the query for retrieval purposes. For example:\n",
        "\n",
        "In addition to semantic search, we can build in structured filters (e.g., \"Find documents since the year 2020.\");\n",
        "The model can rewrite user queries, which may be multifaceted or include irrelevant language, into more effective search queries."
      ],
      "metadata": {
        "id": "MbfXEN70aaid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_documents = len(all_splits)\n",
        "third = total_documents // 3\n",
        "print(len(all_splits))\n",
        "for i, document in enumerate(all_splits):\n",
        "    if i < third:\n",
        "        document.metadata[\"section\"] = \"beginning\"\n",
        "    elif i < 2 * third:\n",
        "        document.metadata[\"section\"] = \"middle\"\n",
        "    else:\n",
        "        document.metadata[\"section\"] = \"end\"\n",
        "\n",
        "\n",
        "all_splits[21].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy3MBEZBYbhU",
        "outputId": "e3224162-a720-4378-bab0-688778022d7f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
              " 'start_index': 16535,\n",
              " 'section': 'middle'}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)\n",
        "_ = vector_store.add_documents(all_splits[:5])"
      ],
      "metadata": {
        "id": "fAv1OCYDX9IO"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "from typing_extensions import Annotated\n",
        "\n",
        "\n",
        "class Search(TypedDict):\n",
        "    \"\"\"Search query.\"\"\"\n",
        "\n",
        "    query: Annotated[str, ..., \"Search query to run.\"]\n",
        "    section: Annotated[\n",
        "        Literal[\"beginning\", \"middle\", \"end\"],\n",
        "        ...,\n",
        "        \"Section to query.\",\n",
        "    ]"
      ],
      "metadata": {
        "id": "IbRopIsoZGxx"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# D√©finition de l'√©tat global de l'application\n",
        "class State(TypedDict):\n",
        "    # Question pos√©e par l'utilisateur\n",
        "    question: str\n",
        "    # Query structur√©e extraite de la question (format Search)\n",
        "    query: Search\n",
        "    # Liste des documents r√©cup√©r√©s apr√®s recherche\n",
        "    context: List[Document]\n",
        "    # R√©ponse finale g√©n√©r√©e par le mod√®le\n",
        "    answer: str\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# √âtape 1 : Analyse de la question\n",
        "# -----------------------------\n",
        "def analyze_query(state: State):\n",
        "    # Configure le mod√®le pour produire une sortie structur√©e selon le sch√©ma Search\n",
        "    structured_llm = model.with_structured_output(Search)\n",
        "\n",
        "    # Envoie la question au mod√®le pour g√©n√©rer une query structur√©e\n",
        "    # R√©sultat attendu : {'query': 'texte √† rechercher', 'section': 'beginning/middle/end'}\n",
        "    query = structured_llm.invoke(state[\"question\"])\n",
        "\n",
        "    # Retourne un dictionnaire pour mettre √† jour l'√©tat avec la query\n",
        "    return {\"query\": query}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# √âtape 2 : Recherche dans le vector store\n",
        "# -----------------------------\n",
        "def retrieve(state: State):\n",
        "    # R√©cup√®re la query structur√©e depuis l'√©tat\n",
        "    query = state[\"query\"]\n",
        "\n",
        "    # Recherche les documents les plus pertinents dans le vector store\n",
        "    # - query[\"query\"] ‚Üí texte √† rechercher\n",
        "    # - filter ‚Üí ne garde que les documents correspondant √† la section sp√©cifi√©e\n",
        "    retrieved_docs = vector_store.similarity_search(\n",
        "        query[\"query\"],\n",
        "        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],\n",
        "    )\n",
        "\n",
        "    # Retourne un dictionnaire pour mettre √† jour l'√©tat avec les documents r√©cup√©r√©s\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# √âtape 3 : G√©n√©ration de la r√©ponse\n",
        "# -----------------------------\n",
        "def generate(state: State):\n",
        "    # Concat√®ne le contenu de tous les documents r√©cup√©r√©s pour cr√©er le contexte\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "\n",
        "    # Pr√©pare le prompt en combinant la question et le contexte\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "\n",
        "    # Envoie le prompt au mod√®le pour g√©n√©rer la r√©ponse\n",
        "    response = model.invoke(messages)\n",
        "\n",
        "    # Retourne un dictionnaire pour mettre √† jour l'√©tat avec la r√©ponse\n",
        "    return {\"answer\": response.content}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# √âtape 4 : Construction du graphe d'application\n",
        "# -----------------------------\n",
        "# Cr√©e un graphe d'√©tats bas√© sur la structure State\n",
        "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
        "\n",
        "# Relie le point de d√©part START √† la premi√®re √©tape analyze_query\n",
        "graph_builder.add_edge(START, \"analyze_query\")\n",
        "\n",
        "# Compile le graphe pour qu'il soit pr√™t √† ex√©cuter le pipeline complet\n",
        "graph = graph_builder.compile()\n"
      ],
      "metadata": {
        "id": "pxLMovB4Zi26"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in graph.stream(\n",
        "    {\"question\": \"What about Task Decomposition?\"},\n",
        "    stream_mode=\"updates\",\n",
        "):\n",
        "    print(f\"{step}\\n\\n----------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wVf8_znZmSN",
        "outputId": "12f4caa3-347c-4075-f18d-5dc7be4cb350"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'analyze_query': {'query': {'query': 'Task Decomposition', 'section': 'beginning'}}}\n",
            "\n",
            "----------------\n",
            "\n",
            "{'retrieve': {'context': [Document(id='6c61e624-e371-4729-945f-265cc8f545a5', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 8, 'section': 'beginning'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'), Document(id='180b8ad9-7b8d-4345-b17a-f0b5e77e89bd', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 971, 'section': 'beginning'}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.'), Document(id='09d564a6-f9e1-47ca-8a38-a3ec6cf6b77f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638, 'section': 'beginning'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'), Document(id='be3441f4-1ae9-4145-8464-ee50103d3d72', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 3549, 'section': 'beginning'}, page_content='Self-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)')]}}\n",
            "\n",
            "----------------\n",
            "\n",
            "{'generate': {'answer': 'Task Decomposition is a process in LLM-powered autonomous agents where large tasks are broken down into smaller, manageable subgoals, enabling efficient handling of complex tasks. This is achieved through techniques such as Chain of Thought (CoT) and Tree of Thoughts (Yao et al. 2023), which guide the model to think step by step. By decomposing tasks, agents can improve their performance on complex tasks and shed light on their thinking process.'}}\n",
            "\n",
            "----------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OSr0xZt6awZH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}